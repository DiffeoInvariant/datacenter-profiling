The main purpose of this project is to provide an extensible framework for the use of Extended Berkeley Packet Filter (eBPF)-based network profiling in MPI clusters. If a programmer goes to the effort of writing and debugging an MPI program---or even if the user simply wants to compare among existing programs---they will often care about that program running fast and efficiently on a cluster. Often, network bandwidth is the biggest bottleneck of such programs, particularly in scientific computing, so a programmer interested in a high-performance application will want to be able to monitor their application's network usage, ideally with minimal CPU and network overhead. Libraries like PETSc\footnote{\url{https://www.mcs.anl.gov/petsc/}} enable such tracking easily --- for instance, any PETSc program can be run with the command-line option \lstinline{-log_view} to view a summary of the program's performance statistics, including network usage. However, MPI programmers that are not using PETSc or a similar library generally will have to recompile their application to get a network usage profile or trace from tools like VampirTrace or PMPI. As anyone who has had to compile a large scientific application knows, this is not always as straightforward as it sounds, and can take a long time even when the process goes smoothly. \\
One solution to this problem comes from the modern linux kernel, which enables very low-level tracing of both kernel- and user-space events by attaching eBPF programs to kprobes. \footnote{\url{https://lkml.org/lkml/2015/4/14/232}} The BPF Compiler Collection (BCC)\footnote{https://github.com/iovisor/bcc} provides a set of pre-written eBPF programs that are useful for instrumentation of all sorts, including network profiling. In particular, the programs \lstinline{tcpaccept(8)}, \lstinline{tcpconnect(8)}, \lstinline{tcpconnlat(8)}, \lstinline{tcplife(8)}, and \lstinline{tcpretrans(8)} can be combined to produce a useful summary of a program's network usage. \\
Specifically, if each node in an MPI cluster is running all of the above-named eBPF programs (\lstinline{tcpaccept(8)} and friends), their outputs can be combined together to produce process-wise summaries of TCP traffic, which we store in a hash table whose keys are process ID numbers (PIDs). Periodically, each node pushes its summaries into an MPI-backed message queue, and those queues are sent back to the root node. After receiving those messages containing summaries on other MPI ranks (each running on its own node), the root node writes them to an output file. Additionally, as soon as data is available, the root node launches a Flask server that serves requests to a REST API that allows the user to request all collected data, all data on a certain MPI rank with a given PID, or all data for all processes (running on any MPI rank) with a given name.
